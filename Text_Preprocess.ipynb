{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkevJXc248JY",
        "outputId": "d6b4e649-495f-4311-80d6-566305e87327"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0ninEao35FT",
        "outputId": "9d31ea92-9558-4e51-a03f-99998c80b831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Libraries imported and 'file.txt' loaded to 'text_corpus' variable.\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from textblob import TextBlob # Used for simple spelling correction\n",
        "\n",
        "corpus_content = \"\"\"\n",
        "The quik brown fox jumpd over the lazy dog. Dogs r often seen as man's best friend. I am going to the store right now to purchas some milk and braed. Machine learning is a feild of study that gives computrs the ability to learn without being explicity programd. This is the fourth and final sentence.\n",
        "\"\"\"\n",
        "# Create and write the corpus content to 'file.txt'\n",
        "with open('file.txt', 'w') as f:\n",
        "    f.write(corpus_content.strip())\n",
        "\n",
        "# Load the text corpus to a variable\n",
        "with open('file.txt', 'r') as f:\n",
        "    text_corpus = f.read()\n",
        "\n",
        "print(\"✅ Libraries imported and 'file.txt' loaded to 'text_corpus' variable.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the tokenization process to the text corpus\n",
        "tokens = word_tokenize(text_corpus)\n",
        "\n",
        "# Print the first 30 tokens\n",
        "print(\"-------Tokens (First 30)---------\")\n",
        "print(tokens[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEyoHfqJ4Eaa",
        "outputId": "a03ea77e-c575-4b96-8ccf-2a13a6e6033f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------Tokens (First 30)---------\n",
            "['The', 'quik', 'brown', 'fox', 'jumpd', 'over', 'the', 'lazy', 'dog', '.', 'Dogs', 'r', 'often', 'seen', 'as', 'man', \"'s\", 'best', 'friend', '.', 'I', 'am', 'going', 'to', 'the', 'store', 'right', 'now', 'to', 'purchas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply spelling correction on each token\n",
        "corrected_tokens = []\n",
        "corrected_text = []\n",
        "\n",
        "# TextBlob is often more effective for whole-text correction\n",
        "blob = TextBlob(text_corpus)\n",
        "corrected_text_corpus = str(blob.correct())\n",
        "\n",
        "# Re-tokenize the corrected text for subsequent steps\n",
        "corrected_tokens = word_tokenize(corrected_text_corpus)\n",
        "\n",
        "# Print the initial 10 corrected tokens\n",
        "print(\"-------- Corrected Tokens (First 10) -------\")\n",
        "print(corrected_tokens[:10])\n",
        "\n",
        "# Print the corrected text corpus\n",
        "print(\"\\n--------- Corrected Text Corpus ----------\")\n",
        "print(corrected_text_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-H3xHtw4HFz",
        "outputId": "f2f42150-b60c-4856-e95d-32b40c1d0e5e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------- Corrected Tokens (First 10) -------\n",
            "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '.']\n",
            "\n",
            "--------- Corrected Text Corpus ----------\n",
            "The quick brown fox jumped over the lazy dog. Dogs r often seen as man's best friend. I am going to the store right now to purchase some milk and bread. Machine learning is a field of study that gives computers the ability to learn without being explicitly program. His is the fourth and final sentence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply POS tags to each corrected token\n",
        "pos_tags = nltk.tag.pos_tag(corrected_tokens)\n",
        "\n",
        "print(\"------- POS Tags (First 15)-----------\")\n",
        "print(pos_tags[:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDIv3RRE4KBg",
        "outputId": "ec76f385-0454-4465-c9a3-db52d053fe2c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------- POS Tags (First 15)-----------\n",
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.'), ('Dogs', 'NNP'), ('r', 'NN'), ('often', 'RB'), ('seen', 'VBN'), ('as', 'IN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words from the corrected token list\n",
        "# Get the English stop words list\n",
        "english_stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Filter the corrected tokens\n",
        "filtered_tokens = [token for token in corrected_tokens if token.lower() not in english_stop_words and token.isalpha()]\n",
        "\n",
        "# Print the initial 20 tokens\n",
        "print(\"---------Filtered Tokens (Stop Words Removed - First 20)--------------\")\n",
        "print(filtered_tokens[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRIT67sN4NHw",
        "outputId": "a40625cc-f83b-449b-eef0-1f91ac0a9345"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------Filtered Tokens (Stop Words Removed - First 20)--------------\n",
            "['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog', 'Dogs', 'r', 'often', 'seen', 'man', 'best', 'friend', 'going', 'store', 'right', 'purchase', 'milk', 'bread', 'Machine']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply stemming and lemmatization to the corrected token list\n",
        "\n",
        "# Initialize the stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "\n",
        "print(\"------------Stemmed Tokens (First 20)------------\")\n",
        "print(stemmed_tokens[:20])\n",
        "\n",
        "print(\"\\n----------Lemmatized Tokens (First 20)----------\")\n",
        "print(lemmatized_tokens[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z24GMAGL4PVD",
        "outputId": "9d44ebb2-9a50-4c21-c31b-04aa01819497"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------Stemmed Tokens (First 20)------------\n",
            "['quick', 'brown', 'fox', 'jump', 'lazi', 'dog', 'dog', 'r', 'often', 'seen', 'man', 'best', 'friend', 'go', 'store', 'right', 'purchas', 'milk', 'bread', 'machin']\n",
            "\n",
            "----------Lemmatized Tokens (First 20)----------\n",
            "['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog', 'Dogs', 'r', 'often', 'seen', 'man', 'best', 'friend', 'going', 'store', 'right', 'purchase', 'milk', 'bread', 'Machine']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect the sentence boundaries in the given text corpus\n",
        "# Use the sent_tokenize function from NLTK\n",
        "sentences = sent_tokenize(text_corpus)\n",
        "\n",
        "# Print the total number of sentences\n",
        "total_sentences = len(sentences)\n",
        "\n",
        "print(\"---------Sentence Boundary Detection-----------\")\n",
        "print(f\"Total number of sentences detected: <<<{total_sentences}>>>\")\n",
        "print(\"\\nAll Sentences:\")\n",
        "for i, sent in enumerate(sentences):\n",
        "    print(f\"{i+1}. {sent}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQlIK1En4Rlm",
        "outputId": "0ce6bcab-386c-4a04-9bec-74b1c5381b7b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------Sentence Boundary Detection-----------\n",
            "Total number of sentences detected: <<<5>>>\n",
            "\n",
            "All Sentences:\n",
            "1. The quik brown fox jumpd over the lazy dog.\n",
            "2. Dogs r often seen as man's best friend.\n",
            "3. I am going to the store right now to purchas some milk and braed.\n",
            "4. Machine learning is a feild of study that gives computrs the ability to learn without being explicity programd.\n",
            "5. This is the fourth and final sentence.\n"
          ]
        }
      ]
    }
  ]
}