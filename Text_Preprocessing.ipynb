{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlvSTMQKGU2p",
        "outputId": "177bba95-a476-4dd1-c733-3991dca3a387"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install autocorrect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAooejH8IgHR",
        "outputId": "6d151e0d-5527-417e-8ad8-ba16ca3914ea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: autocorrect in /usr/local/lib/python3.12/dist-packages (2.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BK8qERcwDqZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cd62d31-92ac-4d99-aaa4-e7984c5f6a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The movie was soooo boring. don’t waste your time!.Best purchase ever!! Highly recommend.This item broke after 2 days. Disappointed.\n",
            "The quik brown fox jumpd over the lazy dog. Dogs r often seen as man's best friend. I am going to the store right now to purchas some milk and braed. Machine learning is a feild of study that gives computrs the ability to learn without being explicity programd. This is the fourth and final sentence.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from autocorrect import Speller\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "f = open('/content/f1.txt')\n",
        "raw = f.read()\n",
        "print(raw)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(raw)\n",
        "print(f\"Initial 30 tokens: {words[:30]}\")"
      ],
      "metadata": {
        "id": "qx6gVoupFtup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "793008d0-92be-42f0-c520-9346d01e2cf4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial 30 tokens: ['The', 'movie', 'was', 'soooo', 'boring', '.', 'don', '’', 't', 'waste', 'your', 'time', '!', '.Best', 'purchase', 'ever', '!', '!', 'Highly', 'recommend.This', 'item', 'broke', 'after', '2', 'days', '.', 'Disappointed', '.', 'The', 'quik']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spell_checker = Speller()\n",
        "corrected_tokens = [spell_checker(word) for word in words]\n",
        "print(f\"Initial 10 corrected tokens: {corrected_tokens[:10]}\")\n",
        "\n",
        "corrected_text_corpus = ' '.join(corrected_tokens)\n",
        "print(f\"Corrected text corpus snippet (first 100 characters): {corrected_text_corpus[:100]}...\")"
      ],
      "metadata": {
        "id": "0rmYX5HYFz8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef05aa2a-a53b-4ec0-f9fc-23b10b5049ff"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial 10 corrected tokens: ['The', 'movie', 'was', 'soon', 'boring', '.', 'don', '’', 't', 'waste']\n",
            "Corrected text corpus snippet (first 100 characters): The movie was soon boring . don ’ t waste your time ! .Best purchase ever ! ! Highly recommend.This ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tagged_tokens = nltk.pos_tag(corrected_tokens)\n",
        "print(f\"POS tagged corrected tokens (initial 10): {pos_tagged_tokens[:10]}\")"
      ],
      "metadata": {
        "id": "nbmlkWzLF0j1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6283dc05-d926-4604-c31b-14557b321d4c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tagged corrected tokens (initial 10): [('The', 'DT'), ('movie', 'NN'), ('was', 'VBD'), ('soon', 'RB'), ('boring', 'JJ'), ('.', '.'), ('don', 'VB'), ('’', 'JJ'), ('t', 'JJ'), ('waste', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [w for w in corrected_tokens if w.lower() not in stop_words]\n",
        "print(f\"Filtered tokens (initial 20, after stop word removal): {filtered_tokens[:20]}\")"
      ],
      "metadata": {
        "id": "gOCCYaB4GHZ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "770e0486-549b-4470-ddca-c01fea598c00"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered tokens (initial 20, after stop word removal): ['movie', 'soon', 'boring', '.', '’', 'waste', 'time', '!', '.Best', 'purchase', 'ever', '!', '!', 'Highly', 'recommend.This', 'item', 'broke', '2', 'days', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p_stemmer = PorterStemmer()\n",
        "stemmed_tokens = [p_stemmer.stem(w) for w in filtered_tokens]\n",
        "print(f\"Stemmed tokens (initial 20): {stemmed_tokens[:20]}\")"
      ],
      "metadata": {
        "id": "ZXlMlFZgGIIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d84fe813-2548-4ea5-fcc3-055492fdadd7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed tokens (initial 20): ['movi', 'soon', 'bore', '.', '’', 'wast', 'time', '!', '.best', 'purchas', 'ever', '!', '!', 'highli', 'recommend.thi', 'item', 'broke', '2', 'day', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
        "print(f\"Lemmatized tokens (initial 20): {lemmatized_tokens[:20]}\")"
      ],
      "metadata": {
        "id": "V_9ewUE2GMpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc41008e-cb1a-40c8-a9d4-8498873fb80f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized tokens (initial 20): ['movie', 'soon', 'boring', '.', '’', 'waste', 'time', '!', '.Best', 'purchase', 'ever', '!', '!', 'Highly', 'recommend.This', 'item', 'broke', '2', 'day', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(raw)\n",
        "total_sentences = len(sentences)\n",
        "print(f\"Total number of sentences detected in the text corpus: {total_sentences}\")"
      ],
      "metadata": {
        "id": "2wS2NKvuGNa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de9b4753-2e98-4a03-f9d7-ca829f9cf4bf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of sentences detected in the text corpus: 9\n"
          ]
        }
      ]
    }
  ]
}